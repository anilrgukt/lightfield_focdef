{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.16"
    },
    "colab": {
      "name": "ulf_focdef_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NoSmPIVv1QG",
        "colab_type": "text"
      },
      "source": [
        "###  **Module imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdKg_6fgPNlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import h5py as h5\n",
        "from scipy import io\n",
        "from scipy.misc import imsave\n",
        "from scipy import interpolate\n",
        "from scipy import ndimage\n",
        "from scipy.io import savemat, loadmat\n",
        "from os import listdir\n",
        "import random\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.measure import compare_ssim as ssim\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/media/flash/ExTra/ulf_focdef/tools/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90Oeeqt2v7MO",
        "colab_type": "text"
      },
      "source": [
        "###  **Light field parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TijVdUbiPNlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#parameters\n",
        "lfsize = [375, 540, 7, 7] #dimensions of Lytro light fields\n",
        "patchsize = [210, 210] #spatial dimensions of training light fields\n",
        "disp_mult = 10.0 #max disparity between adjacent veiws"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_S-4PxDPNlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getLf(data,lfsize):\n",
        "    B,c,H,W = data.shape\n",
        "    v = lfsize[3]\n",
        "    u = lfsize[2]\n",
        "    lf = data.reshape(B,-1,3,W,H)\n",
        "    lf = lf.reshape(B,v,u,3,W,H)\n",
        "    lf = lf.transpose(0,5,4,1,2,3) # B, H, W, v, u, 3\n",
        "\n",
        "    return lf\n",
        "\n",
        "def lfsave(path,idx,key,img):\n",
        "    path = path + str(idx).zfill(3)\n",
        "    if not os.path.isdir(path):\n",
        "        os.mkdir(path)\n",
        "\n",
        "    path = path+'/'+key+'_'\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            imsave(path+str(i)+str(j)+'.png',img[i,j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHStOAmVwD5r",
        "colab_type": "text"
      },
      "source": [
        "###  **Forward warping of light field**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmGhWI5tPNla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#render light field from input image and ray depths\n",
        "\n",
        "def depth_rendering(central, ray_depths, lfsize):\n",
        "    with tf.variable_scope('depth_rendering') as scope:\n",
        "        b_sz = tf.shape(central)[0]\n",
        "        y_sz = tf.shape(central)[1]\n",
        "        x_sz = tf.shape(central)[2]\n",
        "        u_sz = lfsize[2]\n",
        "        v_sz = lfsize[3]\n",
        "        \n",
        "        central = tf.expand_dims(tf.expand_dims(central, 3), 4)\n",
        "                                                \n",
        "        #create and reparameterize light field grid\n",
        "        b_vals = tf.to_float(tf.range(b_sz))\n",
        "        v_vals = tf.to_float(tf.range(v_sz)) - tf.to_float(v_sz/2)\n",
        "        u_vals = tf.to_float(tf.range(u_sz)) - tf.to_float(u_sz/2)\n",
        "        y_vals = tf.to_float(tf.range(y_sz))\n",
        "        x_vals = tf.to_float(tf.range(x_sz))\n",
        "    \n",
        "        b, y, x, v, u = tf.meshgrid(b_vals, y_vals, x_vals, v_vals, u_vals, indexing='ij')\n",
        "               \n",
        "        #warp coordinates by ray depths\n",
        "        y_t = y + v * ray_depths\n",
        "        x_t = x + u * ray_depths\n",
        "        \n",
        "        v_r = tf.zeros_like(b)\n",
        "        u_r = tf.zeros_like(b)\n",
        "        \n",
        "        #indices for linear interpolation\n",
        "        b_1 = tf.to_int32(b)\n",
        "        y_1 = tf.to_int32(tf.floor(y_t))\n",
        "        y_2 = y_1 + 1\n",
        "        x_1 = tf.to_int32(tf.floor(x_t))\n",
        "        x_2 = x_1 + 1\n",
        "        v_1 = tf.to_int32(v_r)\n",
        "        u_1 = tf.to_int32(u_r)\n",
        "        \n",
        "        y_1 = tf.clip_by_value(y_1, 0, y_sz-1)\n",
        "        y_2 = tf.clip_by_value(y_2, 0, y_sz-1)\n",
        "        x_1 = tf.clip_by_value(x_1, 0, x_sz-1)\n",
        "        x_2 = tf.clip_by_value(x_2, 0, x_sz-1)\n",
        "        \n",
        "        #assemble interpolation indices\n",
        "        interp_pts_1 = tf.stack([b_1, y_1, x_1, v_1, u_1], -1)\n",
        "        interp_pts_2 = tf.stack([b_1, y_2, x_1, v_1, u_1], -1)\n",
        "        interp_pts_3 = tf.stack([b_1, y_1, x_2, v_1, u_1], -1)\n",
        "        interp_pts_4 = tf.stack([b_1, y_2, x_2, v_1, u_1], -1)\n",
        "        \n",
        "        #gather light fields to be interpolated\n",
        "        lf_1 = tf.gather_nd(central, interp_pts_1)\n",
        "        lf_2 = tf.gather_nd(central, interp_pts_2)\n",
        "        lf_3 = tf.gather_nd(central, interp_pts_3)\n",
        "        lf_4 = tf.gather_nd(central, interp_pts_4)\n",
        "        \n",
        "        #calculate interpolation weights\n",
        "        y_1_f = tf.to_float(y_1)\n",
        "        x_1_f = tf.to_float(x_1)\n",
        "        d_y_1 = 1.0 - (y_t - y_1_f)\n",
        "        d_y_2 = 1.0 - d_y_1\n",
        "        d_x_1 = 1.0 - (x_t - x_1_f)\n",
        "        d_x_2 = 1.0 - d_x_1\n",
        "        \n",
        "        w1 = d_y_1 * d_x_1\n",
        "        w2 = d_y_2 * d_x_1\n",
        "        w3 = d_y_1 * d_x_2\n",
        "        w4 = d_y_2 * d_x_2\n",
        "        \n",
        "        lf = tf.add_n([w1*lf_1, w2*lf_2, w3*lf_3, w4*lf_4])\n",
        "                        \n",
        "    return lf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9Q1d2ZNwQph",
        "colab_type": "text"
      },
      "source": [
        "###  **Functions for CNN layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14WrwnfaPNld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def weight_variable(w_shape, name):\n",
        "    return tf.get_variable(name, w_shape, initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
        "\n",
        "def bias_variable(b_shape, init_bias=0.0):\n",
        "    return tf.get_variable('bias', b_shape, initializer=tf.constant_initializer(init_bias))\n",
        "\n",
        "#CNN layer no activation\n",
        "def cnn_layer_no_act(input_tensor, w_shape, b_shape, layer_name, is_training, rate=1, padding_type='SAME'):\n",
        "    with tf.variable_scope(layer_name):\n",
        "        W = weight_variable(w_shape, '_weights')\n",
        "        h = tf.nn.atrous_conv2d(input_tensor, W, rate, padding=padding_type, name=layer_name + '_conv')\n",
        "        h = h + bias_variable(b_shape)\n",
        "        return h\n",
        "\n",
        "#Batch normalization \n",
        "def bn(input_tensor, is_training, name='instance_normalization'):\n",
        "    with tf.variable_scope(name):\n",
        "        depth = input_tensor.get_shape()[-1]\n",
        "        scale = tf.get_variable(\n",
        "            'scale', [depth],\n",
        "            initializer=tf.random_normal_initializer(1.0, 0.02, dtype=tf.float32))\n",
        "        offset = tf.get_variable(\n",
        "            'offset', [depth], initializer=tf.constant_initializer(0.0))\n",
        "        mean, variance = tf.nn.moments(input_tensor, axes=[1, 2], keep_dims=True)\n",
        "        epsilon = 1e-5\n",
        "        inv = tf.rsqrt(variance + epsilon)\n",
        "        normalized = (input_tensor - mean) * inv\n",
        "        return scale * normalized + offset\n",
        "    \n",
        "#standard atrous layer\n",
        "def cnn_layer(input_tensor, w_shape, b_shape, layer_name, is_training, rate=1, padding_type='SAME'):\n",
        "    with tf.variable_scope(layer_name):\n",
        "        W = weight_variable(w_shape, '_weights')\n",
        "        h = tf.nn.atrous_conv2d(input_tensor, W, rate, padding=padding_type, name=layer_name + '_conv')\n",
        "        h = h + bias_variable(b_shape)\n",
        "        h = tf.nn.elu(h)\n",
        "        h = tf.contrib.layers.batch_norm(h, scale=True, updates_collections=None, \n",
        "                                             is_training=is_training, scope=layer_name + '_bn')\n",
        "        #h = bn(h, is_training=is_training, name=layer_name + '_bn')\n",
        "        return h\n",
        "    \n",
        "#layer with no normalization or activation\n",
        "def cnn_layer_no_bn(input_tensor, w_shape, b_shape, layer_name, stride=1, padding_type='SAME'):\n",
        "    with tf.variable_scope(layer_name):\n",
        "        W = weight_variable(w_shape, '_weights')\n",
        "        h = tf.nn.conv2d(input_tensor, W, strides=[1, stride, stride, 1], padding=padding_type, name=layer_name + '_conv')\n",
        "        h = h + bias_variable(b_shape)\n",
        "        return h\n",
        "\n",
        "#CNN layer with activation no batch norm\n",
        "def cnn_layer_act_nobn(input_tensor, w_shape, b_shape, layer_name, is_training, rate=1, padding_type='SAME'):\n",
        "    with tf.variable_scope(layer_name):\n",
        "        W = weight_variable(w_shape, '_weights')\n",
        "        h = tf.nn.atrous_conv2d(input_tensor, W, rate, padding=padding_type, name=layer_name + '_conv')\n",
        "        h = h + bias_variable(b_shape)\n",
        "        h = tf.nn.elu(h)\n",
        "        return h\n",
        "\n",
        "#3D convolutional layer with activation and batch norm\n",
        "def cnn_layer3D(input_tensor, w_shape, b_shape, layer_name, is_training, padding_type='SAME'):\n",
        "    with tf.variable_scope(layer_name):\n",
        "        W = weight_variable(w_shape, '_weights')\n",
        "        h = tf.nn.conv3d(input_tensor, W, strides=[1,1,1,1,1], padding=padding_type, name=layer_name + '_conv')\n",
        "        h = h + bias_variable(b_shape)\n",
        "        h = tf.nn.elu(h)\n",
        "        h = tf.contrib.layers.batch_norm(h, scale=True, updates_collections=None, \n",
        "                                             is_training=is_training, scope=layer_name + '_bn')\n",
        "        #h = bn(h, is_training=is_training, name=layer_name + '_bn')\n",
        "        return h\n",
        "\n",
        "#3D convolutional layer no activation no batch norm\n",
        "def cnn_layer3D_no_bn(input_tensor, w_shape, b_shape, layer_name, padding_type='SAME'):\n",
        "    with tf.variable_scope(layer_name):\n",
        "        W = weight_variable(w_shape, '_weights')\n",
        "        h = tf.nn.conv3d(input_tensor, W, strides=[1,1,1,1,1], padding=padding_type, name=layer_name + '_conv')\n",
        "        h = h + bias_variable(b_shape)\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MNcjw2vwntb",
        "colab_type": "text"
      },
      "source": [
        "###  **Depth estimation network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5ZqqbUMPNlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def depth_network(x, xc, lfsize, disp_mult, is_training, name):\n",
        "    with tf.variable_scope(name):\n",
        "        \n",
        "        b_sz = tf.shape(x)[0]\n",
        "        y_sz = tf.shape(x)[1]\n",
        "        x_sz = tf.shape(x)[2]\n",
        "        v_sz = lfsize[2]\n",
        "        u_sz = lfsize[3]\n",
        "        \n",
        "        net_in = tf.concat([x,xc],axis=3)\n",
        "        c1 = cnn_layer(net_in, [3, 3, 6, 16], [16], 'c1', is_training)\n",
        "        c2 = cnn_layer(c1, [3, 3, 16, 64], [64], 'c2', is_training)\n",
        "        c3 = cnn_layer(c2, [3, 3, 64, 128], [128], 'c3', is_training)\n",
        "        c4 = cnn_layer(c3, [3, 3, 128, 128], [128], 'c4', is_training, rate=2)\n",
        "        c5 = cnn_layer(c4, [3, 3, 128, 128], [128], 'c5', is_training, rate=2)\n",
        "        c6 = cnn_layer(c5, [3, 3, 128, 128], [128], 'c6', is_training, rate=4)\n",
        "        c7 = cnn_layer(c6, [3, 3, 128, 128], [128], 'c7', is_training, rate=8)\n",
        "        c8 = cnn_layer(c7, [3, 3, 128, 64], [64], 'c8', is_training, rate=16)\n",
        "\n",
        "        sc1 = cnn_layer(c3, [3, 3, 128, 128], [128], 'sc1', is_training)\n",
        "        sc2 = cnn_layer(sc1, [3, 3, 128, 128], [128], 'sc2', is_training)\n",
        "        sc3 = cnn_layer(sc2, [3, 3, 128, 64], [64], 'sc3', is_training)\n",
        "        \n",
        "        dsc1 = cnn_layer(c6, [3, 3, 128, 64], [64], 'dsc1', is_training)\n",
        "        dsc2 = cnn_layer(dsc1, [3, 3, 64, 64], [64], 'dsc2', is_training)\n",
        "        \n",
        "        dsc3 = cnn_layer(c7, [3, 3, 128, 32], [32], 'dsc3', is_training)        \n",
        "        dsc4 = cnn_layer(dsc3, [3, 3, 32, 32], [32], 'dsc4', is_training)        \n",
        "        \n",
        "        dsc5 = cnn_layer(c8, [3, 3, 64, 32], [32], 'dsc5', is_training)        \n",
        "        dsc6 = cnn_layer(dsc5, [3, 3, 32, 32], [32], 'dsc6', is_training)        \n",
        "        \n",
        "        concat_feat = tf.concat([sc3,dsc2,dsc4,dsc6],axis=3)\n",
        "        \n",
        "        c13 = cnn_layer(concat_feat, [3, 3, 192, 128], [128], 'c13', is_training)\n",
        "        c14 = cnn_layer(c13, [3, 3, 128, 128], [128], 'c14', is_training)\n",
        "        c15 = cnn_layer(c14, [3, 3, 128, 49], [49], 'c15', is_training)\n",
        "        c12 = cnn_layer(c15, [3, 3, 49, 49], [49], 'c12', is_training)\n",
        "        c16 = cnn_layer(c12, [3, 3, 49, lfsize[2]*lfsize[3]], [lfsize[2]*lfsize[3]], 'c16', is_training)\n",
        "        c17 = disp_mult*tf.tanh(cnn_layer_no_bn(c16, [3, 3, lfsize[2]*lfsize[3], lfsize[2]*lfsize[3]], \n",
        "                                                [lfsize[2]*lfsize[3]], 'c10'))\n",
        "        \n",
        "        return tf.reshape(c17, [b_sz, y_sz, x_sz, v_sz, u_sz], name='rayd')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_O2CwqYwtko",
        "colab_type": "text"
      },
      "source": [
        "###  **Light field refinement network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_rBOoh-PNli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def occlusions_network3D_2(d, xc, shear, lfsize, is_training, name):\n",
        "    with tf.variable_scope(name):\n",
        "        \n",
        "        b_sz = tf.shape(d)[0]\n",
        "        y_sz = tf.shape(d)[1]\n",
        "        x_sz = tf.shape(d)[2]\n",
        "        v_sz = lfsize[2]\n",
        "        u_sz = lfsize[3]\n",
        "        #depth\n",
        "        d = tf.tile(tf.expand_dims(d,5), [1,1,1,1,1,3])\n",
        "        d = tf.reshape(d, [b_sz, y_sz, x_sz, v_sz*u_sz, 3])\n",
        "        #light field\n",
        "        x = tf.reshape(shear, [b_sz, y_sz, x_sz, v_sz*u_sz, 3])\n",
        "        #defocused image\n",
        "        xc = tf.expand_dims(xc,3)\n",
        "        #concatenated light field, depth and defocused image as input\n",
        "        xdc = tf.concat([x, d, xc],axis=3)\n",
        "        \n",
        "        \n",
        "        xdc = tf.transpose(xdc, [0, 4, 1, 2, 3]) # B, C, H, W, 2*v*u + 1\n",
        "        #shear = tf.reshape(shear, [b_sz, y_sz, x_sz, v_sz*u_sz*3])\n",
        "        #[filter_depth, filter_height, filter_width, in_channels, out_channels]\n",
        "        c1 = cnn_layer3D(xdc, [3,3,3,99,98], [98], 'c1', is_training, padding_type='SAME')\n",
        "        c2 = cnn_layer3D(c1, [3,3,3,98,98], [98], 'c2', is_training, padding_type='SAME')\n",
        "        c3 = cnn_layer3D(c2, [3,3,3,98,49], [49], 'c3', is_training, padding_type='SAME')\n",
        "        c6 = cnn_layer3D_no_bn(c3, [3, 3, 3, v_sz*u_sz, v_sz*u_sz], [v_sz*u_sz], 'c6', padding_type='SAME')\n",
        "        # o - b,3,h,w,49\n",
        "        c7 = tf.transpose(tf.reshape(c6, [b_sz, 3, y_sz, x_sz, v_sz, u_sz]), [0, 2, 3, 4, 5, 1])\n",
        "        c8 = tf.sigmoid(c7 + shear)\n",
        "        \n",
        "        return tf.reshape(c8, [b_sz, y_sz, x_sz, v_sz, u_sz, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP6JvuBdxBoB",
        "colab_type": "text"
      },
      "source": [
        "###  **Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSRIOocpPNll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#full forward model\n",
        "def forward_model(x, xc, lfsize, disp_mult, is_training):\n",
        "    with tf.variable_scope('forward_model', reuse=None) as scope:\n",
        "        #predict ray depths from input image\n",
        "        ray_depths = depth_network(x, xc, lfsize, disp_mult, is_training, 'ray_depths')\n",
        "\n",
        "        #shear input image by predicted ray depths to render Lambertian light field\n",
        "        lf_shear_r = depth_rendering(x[:, :, :, 0], ray_depths, lfsize)\n",
        "        lf_shear_g = depth_rendering(x[:, :, :, 1], ray_depths, lfsize)\n",
        "        lf_shear_b = depth_rendering(x[:, :, :, 2], ray_depths, lfsize)\n",
        "        lf_shear = tf.stack([lf_shear_r, lf_shear_g, lf_shear_b], axis=5)\n",
        "\n",
        "        #occlusion/non-Lambertian prediction network\n",
        "        d = tf.stop_gradient(ray_depths)\n",
        "        lfs = tf.stop_gradient(lf_shear)\n",
        "        #y, yr = occlusions_network3D_1(d, xc, lfs, lfsize, is_training, 'occlusions')#occlusions\n",
        "        y = occlusions_network3D_2(d, xc, lfs, lfsize, is_training, 'occ2')#occlusions\n",
        "        return ray_depths, lf_shear, y, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKESohVyPNlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_lf(lf):\n",
        "    return lf "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkObRCvoPNlq",
        "colab_type": "code",
        "colab": {},
        "outputId": "84a40fd1-edec-41d5-b2c8-a600c4c2545f"
      },
      "source": [
        "is_training = tf.placeholder(tf.bool, [],name='trainswitch')\n",
        "x_batch = tf.placeholder(tf.float32,[None,patchsize[0],patchsize[1],3],name='cv')\n",
        "xc_batch = tf.placeholder(tf.float32,[None,patchsize[0],patchsize[1],3],name='coded')\n",
        "lf_batch = tf.placeholder(tf.float32,[None,patchsize[0],patchsize[1],lfsize[2],lfsize[3],3],name='lf')\n",
        "\n",
        "batchsize = 1\n",
        "code = np.ones([1,1,7,7,1])\n",
        "code = code[np.newaxis,:,:,:,:,:]/49.0\n",
        "code = np.tile(code,(1,patchsize[0],patchsize[1],1,1,3))\n",
        "\n",
        "print(code.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 210, 210, 7, 7, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBMOHR7mxNOU",
        "colab_type": "text"
      },
      "source": [
        "###  **Forward model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEQigg8dPNls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ray_depths, lf_shear, y, yr = forward_model(x_batch, xc_batch, lfsize, disp_mult, is_training)\n",
        "\n",
        "#losses\n",
        "with tf.name_scope('loss'):\n",
        "    shear_loss = tf.reduce_mean(tf.abs(lf_shear-lf_batch))\n",
        "    output_loss = tf.reduce_mean(tf.abs(y-lf_batch)) \n",
        "    train_loss = shear_loss + output_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUUanNhQxRji",
        "colab_type": "text"
      },
      "source": [
        "###  **PSNR and SSIM evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwLwLZRqPNlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_ssim(refLf, genLf):\n",
        "    ss = np.zeros([refLf.shape[0], refLf.shape[1], refLf.shape[2]])\n",
        "\n",
        "    for i in range(refLf.shape[1]):\n",
        "        for j in range(refLf.shape[2]):\n",
        "            for k in range(refLf.shape[0]):\n",
        "                refimg = (refLf[k,i,j,:,:,:]*255).astype(np.uint8)\n",
        "                genimg = (genLf[k,i,j,:,:,:]*255).astype(np.uint8)\n",
        "                #print refimg., genimg.astype(float64)\n",
        "                #print type(refimg), type(genimg), refimg.shape, genimg.shape\n",
        "                ss[k,i,j] = ssim(refimg, genimg, data_range=255.0, multichannel=True)\n",
        "    ss = ss.sum(0)/float(refLf.shape[0])\n",
        "    return ss\n",
        "\n",
        "def psnr_eval(gt,rec):\n",
        "    gt =  (gt*255).astype(np.uint8)\n",
        "    rec = (rec*255).astype(np.uint8)\n",
        "    # v,u,h,w,3\n",
        "    psnr_val = -10*np.log10(np.mean(pow(gt-rec,2), axis=(2,3,4))) + 20*np.log10(255.0) # v,u\n",
        "    psnr_val[3,3] = 0\n",
        "    return psnr_val\n",
        "\n",
        "def all_eval(Lf, lf_rec, lf_y):\n",
        "\n",
        "    plf = psnr_eval(Lf,lf_rec)\n",
        "    plfy = psnr_eval(Lf,lf_y)\n",
        "\n",
        "    l1 = np.mean(np.absolute(Lf-lf_rec), axis=(2,3,4))\n",
        "    l1[3,3] = 0\n",
        "\n",
        "    l1y = np.mean(np.absolute(Lf-lf_y), axis=(2,3,4))\n",
        "    l1y[3,3] = 0\n",
        "\n",
        "    ss = eval_ssim(Lf[np.newaxis, ...], lf_rec[np.newaxis, ...])\n",
        "    ss[3,3] = 0\n",
        "\n",
        "    ssy = eval_ssim(Lf[np.newaxis, ...], lf_y[np.newaxis, ...])\n",
        "    ssy[3,3] = 0\n",
        "\n",
        "    return plf, plfy, l1, l1y, ss, ssy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz-qht-dxXD_",
        "colab_type": "text"
      },
      "source": [
        "###  **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "RvFf2g_ePNl0",
        "colab_type": "code",
        "colab": {},
        "outputId": "4f8ee085-e3bd-4151-acae-651bd3cd448e"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "datapath = '/media/flash/ExTra/ulf_focdef/data/TestSet/PAPER/' # datapath for test light fields\n",
        "img_list = listdir(datapath)\n",
        "\n",
        "\n",
        "img_list.sort()\n",
        "\n",
        "EXTRA=False\n",
        "FULL=True\n",
        "\n",
        "fname = 'focdef_test/'\n",
        "if not os.path.isdir(fname):\n",
        "    os.mkdir(fname)\n",
        "path = fname\n",
        "\n",
        "oP = patchsize[0] # output patch size from the network\n",
        "P = patchsize[0] # patch size\n",
        "odP = patchsize[0] # output disparity patches\n",
        "crop = 15\n",
        "overlap = 15\n",
        "\n",
        "#helper functions for patch operations\n",
        "from readIlliumImages_v1 import illiumTools as tools\n",
        "tools = tools()\n",
        "tools.verbosity = 0\n",
        "tools.p = P\n",
        "tools.s = P - 2*crop - overlap\n",
        "tools.op = oP\n",
        "tools.coP = P - 2*crop # 30 - patch size 60, 90 - for patch size 120 \n",
        "tools.angRes = lfsize[2]\n",
        "tools.viewH = lfsize[0]\n",
        "tools.viewW = lfsize[1]\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth=True\n",
        "\n",
        "with tf.Session(config = config) as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, '/media/flash/ExTra/ulf_focdef/trained_models/model.ckpt-15999') #restoring model\n",
        "        \n",
        "    losss = []; slosss = []\n",
        "    plist = []; pylist = []\n",
        "    slist = []; sylist = []\n",
        "    l1list = []; l1ylist = []\n",
        "\n",
        "    for img in img_list:\n",
        "        #img = img_list[i]\n",
        "        if img[-4:] != '.png':\n",
        "            print('not an image')\n",
        "            continue\n",
        "            \n",
        "        img_num = img_list.index(img)\n",
        "        \n",
        "        print('processing LF img', img, img_num)\n",
        "        img_name = datapath+img\n",
        "        \n",
        "        value = tf.read_file(img_name)\n",
        "        lf = tf.image.decode_png(value, channels=3, dtype=tf.uint16)\n",
        "        lf = tf.to_float(lf[:lfsize[0]*14, :lfsize[1]*14, :])/65535.0\n",
        "        #lf = tf.image.adjust_gamma(lf, gamma=0.4)\n",
        "        #light field preprocessing\n",
        "        lf = tf.image.rgb_to_hsv(tf.pow(lf,1/1.5))\n",
        "        lf = tf.concat([lf[:,:,0:1],lf[:,:,1:2]*1.5,lf[:,:,2:3]],axis=2)\n",
        "        lf = tf.image.hsv_to_rgb(lf)\n",
        "        lf = tf.clip_by_value(lf, 0.0,1.0)\n",
        "        \n",
        "        lf = tf.transpose(tf.reshape(lf, [lfsize[0], 14, lfsize[1], 14, 3], name='img_lf'), [0, 2, 1, 3, 4])\n",
        "        avsz = 8\n",
        "        lf = lf[:, :, (14/2)-(avsz/2):(14/2)+(avsz/2), (14/2)-(avsz/2):(14/2)+(avsz/2), :]\n",
        "        lf = lf[:,:,1:,1:,:]\n",
        "        \n",
        "        \n",
        "        # permute the dim and get patches \n",
        "        Lf = tf.transpose(tf.squeeze(lf),[2,3,0,1,4])\n",
        "        Lf = Lf.eval()\n",
        "        \n",
        "        #extracting patches from light field\n",
        "        pLfs, rlist, clist = tools.extract_patches(Lf) # N,v,v P, P, 3  \n",
        "        pCodedImgs = pLfs.transpose([0,3,4,1,2,5])*code\n",
        "        pCodedImgs = pCodedImgs.sum(-2)\n",
        "        pCodedImgs = pCodedImgs.sum(-2)\n",
        "\n",
        "        # fwd pass of batches \n",
        "        print('fwd ...')\n",
        "        batch_list = range(0,pCodedImgs.shape[0],batchsize)\n",
        "        depths = []; genCv = []; genLf = []; geny = []\n",
        "        #print pLfs.shape\n",
        "        for bid in batch_list:\n",
        "            #print pLfs[bid:bid+batchsize,3,3,3:-3,3:-3,:].shape, pCodedImgs[bid:bid+batchsize,3:-3,3:-3,:].shape, pLfs[bid:bid+batchsize,:,:,3:-3,3:-3,:].shape\n",
        "            feed_dict = {x_batch: pLfs[bid:bid+batchsize,3,3,:,:,:], \n",
        "                         xc_batch: pCodedImgs[bid:bid+batchsize,:,:,:], \n",
        "                         lf_batch: pLfs[bid:bid+batchsize,:,:,:,:,:].transpose([0,3,4,1,2,5]), \n",
        "                        is_training: False}\n",
        "            #x_batch: pLfs[bid:bid+batchsize,3,3,3:-3,3:-3,:],\n",
        "            #Obtain losses, light fields and ray depthds \n",
        "            bray_depths, blf_shear, by, tloss, sloss = sess.run([ray_depths, lf_shear, y, train_loss, shear_loss], \n",
        "                                                         feed_dict=feed_dict)\n",
        "            \n",
        "            depths.append(np.array(bray_depths))\n",
        "            genLf.append(np.array(blf_shear))\n",
        "            geny.append(np.array(by))\n",
        "            \n",
        "            losss.append(tloss)\n",
        "            slosss.append(sloss)\n",
        "            \n",
        "        depths = np.vstack(depths)\n",
        "        genLf = np.vstack(genLf)\n",
        "        geny = np.vstack(geny)\n",
        "        \n",
        "        depths = np.tile(depths[:,:,:,:,:,np.newaxis],(1,1,1,1,1,3))\n",
        "        depths = depths.transpose(0,3,4,1,2,5)\n",
        "        genLf = genLf.transpose(0,3,4,1,2,5)\n",
        "        geny = geny.transpose(0,3,4,1,2,5)\n",
        "        \n",
        "        offset = crop\n",
        "        offr = 9\n",
        "        offc = 0\n",
        "        \n",
        "        #combining patches to generate full light fields and ray depths\n",
        "        if FULL:\n",
        "            warp_reconst = tools.combine_patches(depths, rlist, clist,1)\n",
        "            lf_rec = tools.combine_patches(genLf, rlist, clist,1)\n",
        "            lf_y = tools.combine_patches(geny, rlist, clist,1)\n",
        "            \n",
        "            Lf = Lf[:,:,offset:-offset-offr,offset:-offset-offc,:]\n",
        "            warp_reconst = warp_reconst[:,:,offset:-offset-offr,offset:-offset-offc,:]\n",
        "            lf_rec = lf_rec[:,:,offset:-offset-offr,offset:-offset-offc,:]\n",
        "            lf_y = lf_y[:,:,offset:-offset-offr,offset:-offset-offc,:]\n",
        "            \n",
        "            warp_reconst = warp_reconst[:,:,7:-6,7:-6,:]\n",
        "            Lf = Lf[:,:,7:-6,7:-6,:]\n",
        "            print Lf.shape\n",
        "            lf_rec = lf_rec[:,:,7:-6,7:-6,:]\n",
        "            lf_y = lf_y[:,:,7:-6,7:-6,:]\n",
        "            \n",
        "            #PSNR, SSIM evaluation\n",
        "            plf, plfy, l1, l1y, ss, ssy = all_eval(Lf, lf_rec, lf_y)\n",
        "            \n",
        "            plist.append(plf); pylist.append(plfy)  \n",
        "            l1list.append(l1); l1ylist.append(l1y)\n",
        "            slist.append(ss);  sylist.append(ssy)\n",
        "            \n",
        "            #PSNR, SSIM of unrefined and refined light fields\n",
        "            print('psnr', plf.sum()/48.0, 'psnry', plfy.sum()/48.0)\n",
        "            print('ssim', ss.sum()/48.0,  'ssimy', ssy.sum()/48.0)\n",
        "\n",
        "            plt.imsave(path+str(img_num).zfill(3)+'_bcvdepth.png', warp_reconst[3,3,:,:,0].clip(-2.15,2.15), cmap='plasma')\n",
        "            np.save(path+str(img_num).zfill(3)+'_bcvdepth.npy', warp_reconst[3,3,:,:,:])\n",
        "            imsave(path+str(img_num).zfill(3)+'_cv.png', lf_rec[3,3,:,:,:])\n",
        "            imsave(path+str(img_num).zfill(3)+'_tr.png', lf_rec[0,6,:,:,:])\n",
        "            imsave(path+str(img_num).zfill(3)+'_try.png', lf_y[0,6,:,:,:])\n",
        "            imsave(path+str(img_num).zfill(3)+'_trz.png', Lf[0,6,:,:,:])\n",
        "            \n",
        "            err = np.abs(lf_rec[0,6,:,:,:]-Lf[0,6,:,:,:])\n",
        "            err = err.mean(2)\n",
        "            erry = np.abs(lf_y[0,6,:,:,:]-Lf[0,6,:,:,:])\n",
        "            erry = erry.mean(2)\n",
        "                        \n",
        "            plt.imsave(path+str(img_num).zfill(3)+'_err.png', err.clip(0,0.1), cmap='jet')\n",
        "            plt.imsave(path+str(img_num).zfill(3)+'_erry.png', erry.clip(0,0.1), cmap='jet')\n",
        "\n",
        "        if EXTRA:\n",
        "            #generate only full depths\n",
        "            depths = tools.combine_patches(depths[:,3,3,:,:,:], rlist, clist)\n",
        "            depths = depths[offset:-offset-offr,offset:-offset-offc,:]\n",
        "            depths = depths[7:-6,7:-6,:]\n",
        "            np.save(path+str(img_num).zfill(3)+'_bcvdepth.npy', depths)\n",
        "\n",
        "        codedImg = tools.combine_patches(pCodedImgs, rlist, clist)\n",
        "        codedImg = codedImg[offset:-offset-offr,offset:-offset-offc,:]\n",
        "        codedImg = codedImg[7:-6,7:-6,:]\n",
        "        imsave(path+str(img_num).zfill(3)+'_coded.png', codedImg)\n",
        "    \n",
        "    losss = np.vstack(losss)\n",
        "    print(losss.mean())\n",
        "    losss = np.vstack(slosss)\n",
        "    print(losss.mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /media/flash/ExTra/ulf_focdef/trained_models/model.ckpt-15999\n",
            "processing LF img Cars.png 0\n",
            "fwd ...\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "(7, 7, 323, 497, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/flash/anaconda2/envs/tfv1p12/lib/python2.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log10\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "psnr 35.8107431935 psnry 35.6033650716\n",
            "ssim 0.970317896268 ssimy 0.973363578135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/flash/anaconda2/envs/tfv1p12/lib/python2.7/site-packages/ipykernel_launcher.py:149: DeprecationWarning: `imsave` is deprecated!\n",
            "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imwrite`` instead.\n",
            "/home/flash/anaconda2/envs/tfv1p12/lib/python2.7/site-packages/ipykernel_launcher.py:151: DeprecationWarning: `imsave` is deprecated!\n",
            "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imwrite`` instead.\n",
            "/home/flash/anaconda2/envs/tfv1p12/lib/python2.7/site-packages/ipykernel_launcher.py:152: DeprecationWarning: `imsave` is deprecated!\n",
            "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imwrite`` instead.\n",
            "/home/flash/anaconda2/envs/tfv1p12/lib/python2.7/site-packages/ipykernel_launcher.py:153: DeprecationWarning: `imsave` is deprecated!\n",
            "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imwrite`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "patch shape (6, 210, 210, 3)\n",
            "not lf (6, 210, 210, 3) 15\n",
            "not lf (6, 180, 180, 3)\n",
            "patches shape (6, 180, 180, 3)\n",
            "processing LF img Flower1.png 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/flash/anaconda2/envs/tfv1p12/lib/python2.7/site-packages/ipykernel_launcher.py:182: DeprecationWarning: `imsave` is deprecated!\n",
            "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imwrite`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fwd ...\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "(7, 7, 323, 497, 3)\n",
            "psnr 36.1804210837 psnry 36.1888189691\n",
            "ssim 0.968620180407 ssimy 0.971716505889\n",
            "patch shape (6, 210, 210, 3)\n",
            "not lf (6, 210, 210, 3) 15\n",
            "not lf (6, 180, 180, 3)\n",
            "patches shape (6, 180, 180, 3)\n",
            "processing LF img Flower2.png 2\n",
            "fwd ...\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "(7, 7, 323, 497, 3)\n",
            "psnr 36.1954615077 psnry 35.8245765065\n",
            "ssim 0.962280140585 ssimy 0.962786883115\n",
            "patch shape (6, 210, 210, 3)\n",
            "not lf (6, 210, 210, 3) 15\n",
            "not lf (6, 180, 180, 3)\n",
            "patches shape (6, 180, 180, 3)\n",
            "processing LF img Rock.png 3\n",
            "fwd ...\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "(7, 7, 323, 497, 3)\n",
            "psnr 36.7197636619 psnry 35.7539761313\n",
            "ssim 0.968858329867 ssimy 0.962393214856\n",
            "patch shape (6, 210, 210, 3)\n",
            "not lf (6, 210, 210, 3) 15\n",
            "not lf (6, 180, 180, 3)\n",
            "patches shape (6, 180, 180, 3)\n",
            "processing LF img Seahorse.png 4\n",
            "fwd ...\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "patch shape (6, 7, 7, 210, 210, 3)\n",
            "patches shape (6, 49, 180, 180, 3)\n",
            "(7, 7, 323, 497, 3)\n",
            "psnr 37.7275355902 psnry 37.7993321594\n",
            "ssim 0.968103841766 ssimy 0.972463368971\n",
            "patch shape (6, 210, 210, 3)\n",
            "not lf (6, 210, 210, 3) 15\n",
            "not lf (6, 180, 180, 3)\n",
            "patches shape (6, 180, 180, 3)\n",
            "0.0236778\n",
            "0.0115426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr5ixWXAPNl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plist = np.array(plist)\n",
        "pylist = np.array(pylist)\n",
        "slist = np.array(slist)\n",
        "sylist = np.array(sylist)\n",
        "l1list = np.array(l1list)\n",
        "l1ylist = np.array(l1ylist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBCTYBlzzIJQ",
        "colab_type": "text"
      },
      "source": [
        "###  **PSNR, SSIM and L1 errors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "X3ebc5AcPNl4",
        "colab_type": "code",
        "colab": {},
        "outputId": "a5c085db-d320-4928-a522-9902f0f73827"
      },
      "source": [
        "print (plist.mean(), pylist.mean())\n",
        "print (slist.mean(), sylist.mean())\n",
        "print (l1list.mean(), l1ylist.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzE6GwtFPNl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(path+'plist',plist)\n",
        "np.save(path+'pylist',pylist)\n",
        "np.save(path+'slist',slist)\n",
        "np.save(path+'sylist',sylist)\n",
        "np.save(path+'l1list',l1list)\n",
        "np.save(path+'l1ylist',l1ylist)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
